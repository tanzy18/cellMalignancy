{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "from standardlize import standardlize\n",
    "from equalize import equalize\n",
    "from utils import splitAfterStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/sampleMatrixGene.txt', sep='\\t', index_col=0)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征选择，数据标准化，划分训练测试集\n",
    "selectGenes = np.loadtxt('./selectGenes.txt', dtype=str)\n",
    "selectCol = np.append(selectGenes, ['type','label'])\n",
    "df = df.loc[:,selectCol]\n",
    "\n",
    "data_std = standardlize(df.iloc[:,:-2])\n",
    "train_data , test_data , train_tag , test_tag = splitAfterStd(data_std, df.iloc[:,-1].values, ratio=0.8)\n",
    "train_data_resample , train_tag_resample = equalize(train_data, train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = np.array(train_data_resample)\n",
    "        self.y = np.array(train_tag_resample)\n",
    "        self.x = torch.tensor(self.x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(self.y, dtype=torch.int64)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "class testDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = np.array(test_data)\n",
    "        self.y = np.array(test_tag)\n",
    "        self.x = torch.tensor(self.x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(self.y, dtype=torch.int64)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer6): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=2, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "        super(LinearNet,self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(1000,512), nn.ReLU(True))\n",
    "        self.layer2 = nn.Sequential(nn.Linear(512,256), nn.ReLU(True))\n",
    "        self.layer3 = nn.Sequential(nn.Linear(256,128), nn.ReLU(True))\n",
    "        self.layer4 = nn.Sequential(nn.Linear(128,64), nn.ReLU(True))\n",
    "        self.layer5 = nn.Sequential(nn.Linear(64,32), nn.ReLU(True))\n",
    "        self.layer6 = nn.Sequential(nn.Linear(32,num_classes), nn.ReLU(True))\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        return out\n",
    "        \n",
    "model = LinearNet()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "train_dataset = trainDataSet()\n",
    "test_dataset  = testDataSet()\n",
    "train_loader  = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader   = Data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo: 0\n",
      "loss: 0.691578826858002\n",
      "TrainACC: 0.49883495145631057\n",
      "ValLoss: 0.7008529510991327\n",
      "ValACC: 0.07977011494252877\n",
      "echo: 1\n",
      "loss: 0.6854325035243358\n",
      "TrainACC: 0.5046601941747567\n",
      "ValLoss: 0.6997905846299797\n",
      "ValACC: 0.11770114942528741\n",
      "echo: 2\n",
      "loss: 0.6593773434463056\n",
      "TrainACC: 0.7818446601941749\n",
      "ValLoss: 0.657918749184444\n",
      "ValACC: 0.8648275862068964\n",
      "echo: 3\n",
      "loss: 0.522258539367648\n",
      "TrainACC: 0.9480582524271833\n",
      "ValLoss: 0.44637319548376675\n",
      "ValACC: 0.944137931034483\n",
      "echo: 4\n",
      "loss: 0.19228952475687833\n",
      "TrainACC: 0.9772815533980571\n",
      "ValLoss: 0.13142168264964532\n",
      "ValACC: 0.9737931034482761\n",
      "echo: 5\n",
      "loss: 0.06330715228342315\n",
      "TrainACC: 0.986893203883494\n",
      "ValLoss: 0.06935893647886555\n",
      "ValACC: 0.9806896551724139\n",
      "echo: 6\n",
      "loss: 0.036380845870262066\n",
      "TrainACC: 0.991165048543689\n",
      "ValLoss: 0.05611365795906248\n",
      "ValACC: 0.9841379310344829\n",
      "echo: 7\n",
      "loss: 0.024699154965471006\n",
      "TrainACC: 0.9944660194174753\n",
      "ValLoss: 0.054059336491828335\n",
      "ValACC: 0.9841379310344829\n",
      "echo: 8\n",
      "loss: 0.018951747003599133\n",
      "TrainACC: 0.9957281553398054\n",
      "ValLoss: 0.05296907261624161\n",
      "ValACC: 0.9834482758620691\n",
      "echo: 9\n",
      "loss: 0.015355686085226321\n",
      "TrainACC: 0.997184466019417\n",
      "ValLoss: 0.0510900372896215\n",
      "ValACC: 0.986896551724138\n",
      "echo: 10\n",
      "loss: 0.012402581908848607\n",
      "TrainACC: 0.9978640776699026\n",
      "ValLoss: 0.05080478857993951\n",
      "ValACC: 0.986896551724138\n",
      "echo: 11\n",
      "loss: 0.010433419391702753\n",
      "TrainACC: 0.9978640776699026\n",
      "ValLoss: 0.054923805087034046\n",
      "ValACC: 0.9855172413793104\n",
      "echo: 12\n",
      "loss: 0.010834561309096793\n",
      "TrainACC: 0.9978640776699026\n",
      "ValLoss: 0.05654729141829664\n",
      "ValACC: 0.9875862068965519\n",
      "echo: 13\n",
      "loss: 0.009156822108265772\n",
      "TrainACC: 0.9980582524271843\n",
      "ValLoss: 0.057603219267109346\n",
      "ValACC: 0.9848275862068966\n",
      "echo: 14\n",
      "loss: 0.006876377572499903\n",
      "TrainACC: 0.9985436893203882\n",
      "ValLoss: 0.05533118938810417\n",
      "ValACC: 0.9889655172413794\n",
      "echo: 15\n",
      "loss: 0.005822770620777624\n",
      "TrainACC: 0.9986407766990287\n",
      "ValLoss: 0.057293399365554595\n",
      "ValACC: 0.9889655172413794\n",
      "echo: 16\n",
      "loss: 0.005041013935748126\n",
      "TrainACC: 0.999029126213592\n",
      "ValLoss: 0.05802434178848012\n",
      "ValACC: 0.986896551724138\n",
      "echo: 17\n",
      "loss: 0.004426572697764959\n",
      "TrainACC: 0.999029126213592\n",
      "ValLoss: 0.05901420892342717\n",
      "ValACC: 0.9889655172413794\n",
      "echo: 18\n",
      "loss: 0.003946614540289427\n",
      "TrainACC: 0.9991262135922329\n",
      "ValLoss: 0.058667452158386306\n",
      "ValACC: 0.9889655172413794\n",
      "echo: 19\n",
      "loss: 0.003595429783301582\n",
      "TrainACC: 0.9992233009708736\n",
      "ValLoss: 0.060287924288222085\n",
      "ValACC: 0.9896551724137933\n",
      "echo: 20\n",
      "loss: 0.0031245009997945498\n",
      "TrainACC: 0.9994174757281553\n",
      "ValLoss: 0.06135911344207325\n",
      "ValACC: 0.9896551724137933\n",
      "echo: 21\n",
      "loss: 0.0025258102188354756\n",
      "TrainACC: 0.9996116504854369\n",
      "ValLoss: 0.06403625422058996\n",
      "ValACC: 0.9882758620689657\n",
      "echo: 22\n",
      "loss: 0.002411105305530601\n",
      "TrainACC: 0.999611650485437\n",
      "ValLoss: 0.06420929702816748\n",
      "ValACC: 0.9889655172413796\n",
      "echo: 23\n",
      "loss: 0.002056985155911551\n",
      "TrainACC: 0.9996116504854369\n",
      "ValLoss: 0.06524324907137459\n",
      "ValACC: 0.9889655172413794\n",
      "echo: 24\n",
      "loss: 0.00193984688119652\n",
      "TrainACC: 0.9996116504854367\n",
      "ValLoss: 0.06472272064651217\n",
      "ValACC: 0.9896551724137933\n",
      "echo: 25\n",
      "loss: 0.0016771397298796429\n",
      "TrainACC: 0.9997087378640775\n",
      "ValLoss: 0.06536583034326683\n",
      "ValACC: 0.9896551724137933\n",
      "echo: 26\n",
      "loss: 0.0014534698409953458\n",
      "TrainACC: 0.9997087378640775\n",
      "ValLoss: 0.06636379693322694\n",
      "ValACC: 0.9896551724137933\n",
      "echo: 27\n",
      "loss: 0.0013433356620826265\n",
      "TrainACC: 0.9998058252427184\n",
      "ValLoss: 0.0672929967414379\n",
      "ValACC: 0.9889655172413794\n",
      "echo: 28\n",
      "loss: 0.0012881735709421812\n",
      "TrainACC: 0.9998058252427184\n",
      "ValLoss: 0.06798623734884414\n",
      "ValACC: 0.9896551724137933\n",
      "echo: 29\n",
      "loss: 0.0011291782616044833\n",
      "TrainACC: 0.9999029126213593\n",
      "ValLoss: 0.06904299628362863\n",
      "ValACC: 0.9896551724137933\n",
      "echo: 30\n",
      "loss: 0.0009656123930450757\n",
      "TrainACC: 0.9999029126213592\n",
      "ValLoss: 0.06848342281165092\n",
      "ValACC: 0.9896551724137933\n",
      "echo: 31\n",
      "loss: 0.000929898804757101\n",
      "TrainACC: 0.9999029126213592\n",
      "ValLoss: 0.06919308752950545\n",
      "ValACC: 0.9896551724137933\n",
      "echo: 32\n",
      "loss: 0.0008298569780855509\n",
      "TrainACC: 0.9999029126213592\n",
      "ValLoss: 0.07049649100811402\n",
      "ValACC: 0.9889655172413796\n",
      "echo: 33\n",
      "loss: 0.000768737978909732\n",
      "TrainACC: 0.9999029126213592\n",
      "ValLoss: 0.0707826047074722\n",
      "ValACC: 0.9896551724137933\n",
      "echo: 34\n",
      "loss: 0.0007249116155859174\n",
      "TrainACC: 1.0\n",
      "ValLoss: 0.07149895787608393\n",
      "ValACC: 0.9896551724137933\n",
      "echo: 35\n",
      "loss: 0.0006547659203118828\n",
      "TrainACC: 0.9999029126213592\n",
      "ValLoss: 0.07223387268283565\n",
      "ValACC: 0.9889655172413796\n",
      "echo: 36\n",
      "loss: 0.0006057894028945333\n",
      "TrainACC: 1.0\n",
      "ValLoss: 0.07279928572555444\n",
      "ValACC: 0.9889655172413796\n",
      "echo: 37\n",
      "loss: 0.0005780289082624611\n",
      "TrainACC: 1.0\n",
      "ValLoss: 0.07276939969045182\n",
      "ValACC: 0.9896551724137933\n",
      "echo: 38\n",
      "loss: 0.0005133145610335366\n",
      "TrainACC: 1.0\n",
      "ValLoss: 0.07400169429607119\n",
      "ValACC: 0.9889655172413796\n",
      "echo: 39\n",
      "loss: 0.0005048212510833428\n",
      "TrainACC: 1.0\n",
      "ValLoss: 0.07449026565292774\n",
      "ValACC: 0.9889655172413796\n",
      "echo: 40\n",
      "loss: 0.0004622294280884445\n",
      "TrainACC: 1.0\n",
      "ValLoss: 0.0750690137852432\n",
      "ValACC: 0.9889655172413796\n",
      "echo: 41\n",
      "loss: 0.0004492477089724983\n",
      "TrainACC: 1.0\n",
      "ValLoss: 0.07529341728739633\n",
      "ValACC: 0.9889655172413796\n",
      "echo: 42\n",
      "loss: 0.00039650331839686826\n",
      "TrainACC: 1.0\n",
      "ValLoss: 0.07509241531787948\n",
      "ValACC: 0.9903448275862071\n",
      "echo: 43\n",
      "loss: 0.0004103704561149923\n",
      "TrainACC: 1.0\n",
      "ValLoss: 0.07595295908774052\n",
      "ValACC: 0.9889655172413796\n",
      "echo: 44\n",
      "loss: 0.0003727623925972261\n",
      "TrainACC: 1.0\n",
      "ValLoss: 0.07643757035450484\n",
      "ValACC: 0.9889655172413796\n",
      "echo: 45\n",
      "loss: 0.00036007209492152785\n",
      "TrainACC: 1.0\n",
      "ValLoss: 0.07656337397189607\n",
      "ValACC: 0.9889655172413796\n",
      "echo: 46\n",
      "loss: 0.0003367921626792929\n",
      "TrainACC: 1.0\n",
      "ValLoss: 0.07702173689672079\n",
      "ValACC: 0.9889655172413796\n",
      "echo: 47\n",
      "loss: 0.0003193336255102163\n",
      "TrainACC: 1.0\n",
      "ValLoss: 0.07737842987820652\n",
      "ValACC: 0.9889655172413796\n",
      "echo: 48\n",
      "loss: 0.00030608387895908094\n",
      "TrainACC: 1.0\n",
      "ValLoss: 0.07768257799636621\n",
      "ValACC: 0.9889655172413796\n",
      "echo: 49\n",
      "loss: 0.00028942619560130145\n",
      "TrainACC: 1.0\n",
      "ValLoss: 0.07825141823772795\n",
      "ValACC: 0.9889655172413796\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "LR = 1e-3\n",
    "loss = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=1e-6, nesterov=True)\n",
    "tr_acc = []\n",
    "te_acc = []\n",
    "val_loss_arr = []\n",
    "for echo in range(NUM_EPOCHS):\n",
    "    train_loss = 0   \n",
    "    train_acc = 0   \n",
    "    model.train()    \n",
    "    for i,(X,label) in enumerate(train_loader):    \n",
    "        X = Variable(X).cuda()\n",
    "        # print(X)       \n",
    "        label = Variable(label).cuda()\n",
    "        # print(label)\n",
    "        out = model(X)\n",
    "        # print(out)     \n",
    "        lossvalue = loss(out,label)         \n",
    "        optimizer.zero_grad()       \n",
    "        lossvalue.backward()    \n",
    "        optimizer.step()          \n",
    "         \n",
    "        train_loss += float(lossvalue)   \n",
    "        _,pred = out.max(1)\n",
    "        num_correct = (pred == label).sum()\n",
    "        acc = int(num_correct) / X.shape[0]\n",
    "        train_acc += acc\n",
    "    print(\"echo:\"+' ' + str(echo))\n",
    "    print(\"loss:\" + ' ' + str(train_loss / len(train_loader)))\n",
    "    print(\"TrainACC:\" + ' '+str(train_acc / len(train_loader)))\n",
    "    tr_acc.append(train_acc / len(train_loader))\n",
    "\n",
    "    eval_acc = 0\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    for i,(X,label) in enumerate(test_loader):\n",
    "        X = Variable(X).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        testout = model(X)\n",
    "        v_loss = float(loss(testout,label))\n",
    "\n",
    "        _, pred = testout.max(1)\n",
    "        num_correct = (pred == label).sum()\n",
    "        acc = int(num_correct) / X.shape[0]\n",
    "        eval_acc += acc\n",
    "        val_loss += v_loss\n",
    "    print(\"ValLoss:\" + ' ' + str(val_loss / len(test_loader)))\n",
    "    print('ValACC:' + ' ' + str(eval_acc / len(test_loader)))\n",
    "    cur_eval_acc = eval_acc/len(test_loader)\n",
    "    te_acc.append(eval_acc/len(test_loader))\n",
    "    val_loss_arr.append(val_loss/len(test_loader))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5832b54d6696489189ad5d74cbb0358f697f934322176abd9d7fdf85f5609811"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
